sampv <- rep(0,1000)
for (i in 1:1000) {
sampv[i] <- var(sample(exp,100))
}
mean(sampv)
data <- c(2.0, 1.5, 1.0, 1.2, 1.3, 4.4, 1.0, 4.9, 1.5, 1.2)
log_likelihood <- function(alpha) {
return(10 * log(alpha) - (alpha + 1) * sum(log(data)))
}
alpha_values <- seq(0.1, 10, length.out = 100)
log_likelihood_values <- sapply(alpha_values, log_likelihood)
plot(alpha_values, log_likelihood_values, type = "l",
xlab = "Alpha", ylab = "Log-Likelihood",
main = "Log-Likelihood Function for Alpha")
grid()
data <- c(2.0, 1.5, 1.0, 1.2, 1.3, 4.4, 1.0, 4.9, 1.5, 1.2)
log_likelihood <- function(beta) {
return(10 * log(beta) - (beta + 1) * sum(log(data)))
}
beta_values <- seq(0.1, 10, length.out = 100)
log_likelihood_values <- sapply(beta_values, log_likelihood)
plot(beta_values, log_likelihood_values, type = "l",
xlab = "Beta", ylab = "Log-Likelihood",
main = "Log-Likelihood Function for Beta")
grid()
data <- c(2.0, 1.5, 1.0, 1.2, 1.3, 4.4, 1.0, 4.9, 1.5, 1.2)
log_likelihood <- function(beta) {
return(10 * log(beta) - (beta + 1) * sum(log(data)))
}
beta_values <- seq(0.1, 10, length.out = 100)
log_likelihood_values <- sapply(beta_values, log_likelihood)
plot(beta_values, log_likelihood_values, type = "l",
xlab = "Beta", ylab = "Log-Likelihood",
main = "Log-Likelihood Function of Beta")
grid()
data <- c(2.0, 1.5, 1.0, 1.2, 1.3, 4.4, 1.0, 4.9, 1.5, 1.2)
log_likelihood <- function(beta) {
return(10 * log(beta) - (beta + 1) * sum(log(data)))
}
beta_values <- seq(0.1, 10, length.out = 100)
log_likelihood_values <- sapply(beta_values, log_likelihood)
plot(beta_values, log_likelihood_values, type = "l",
xlab = "Beta", ylab = "Log-Likelihood",
main = "Log-Likelihood Function for Beta")
grid()
result <- optim(par = 1, fn = log_likelihood, method = "Brent", lower = 0.001, upper = 10)
# Extract the maximum likelihood estimate of alpha
beta_star <- result$par
# Print the result
cat("Maximum Likelihood Estimate of Beta*:", beta_star, "\n")
sum(ln(data))
sum(log(data))
10/5.201925
library(readr)
camp_primary <- read_csv("Desktop/JHU/Analysis of multilevel and longitudinal data/Lab2/camp_primary.csv")
View(camp_primary)
dat <- camp_primary
View(dat)
library(tidyverse)
library(nlme)
library(lme4)
# Fit the independence model
mA = gls(POSFEV~year+as.factor(trt):year,data=dat,na.action=na.omit,method="ML")
dat$year <- dat$visitc/12
dat <- dat[visitc<=24,]
dat <- dat[dat$visitc<=24,]
# Fit the independence model
mA = gls(POSFEV~year+as.factor(trt):year,data=dat,na.action=na.omit,method="ML")
# Fit the exchangeable model
mB = gls(POSFEV~year+as.factor(trt):year,data=dat,na.action=na.omit,correlation=corCompSymm(form=~1|id),method="ML")
# Fit the exponential model
mC = gls(POSFEV~year+as.factor(trt):year,data=dat,na.action=na.omit,correlation=corExp(form=~year|id),method="ML")
# Fit the random intercept + exponential within subject correlation structure
mD = lme(POSFEV~year+as.factor(trt):year,data=dat,na.action=na.omit,
random = ~ 1 | id, correlation = corExp(form=~year|id),method="ML")
# Fit the random intercept + slope model
mE = lme(POSFEV~year+as.factor(trt):year,data=dat,na.action=na.omit,
random = ~ 1 + year | id,method="ML")
c(AIC(mA),AIC(mB),AIC(mC),AIC(mD),AIC(mE))
qt(0.95,df=24)
knitr::opts_chunk$set(echo = TRUE)
sampleSize <- 0
for (i in 1:1000){
sampleSize <- 2*qt(0.95, df=i-1)*3/sqrt(i)
if (sampleSize <= 0.5){
break
}
}
sampleSize <- 0
for (i in 2:1000){
sampleSize <- 2*qt(0.95, df=i-1)*3/sqrt(i)
if (sampleSize <= 0.5){
break
}
}
sampleSize
sampleSize <- 0
for (i in 2:1000){
sampleSize[i] <- 2*qt(0.95, df=i-1)*3/sqrt(i)
if (sampleSize[i] <= 0.5){
break
}
}
len(sampleSize)
sampleSize <- 0
for (i in 2:1000){
sampleSize[i] <- 2*qt(0.95, df=i-1)*3/sqrt(i)
if (sampleSize[i] <= 0.5){
break
}
}
length(sampleSize)
qt(0.25,259,288)
qf(0.25,259,288)
qt(0.025,259,288)
qf(0.025,259,288)
qf(0.075,259,288)
qf(0.025,259,288)
qf(0.975,259,288)
qf(0.025,259,288)
qf(0.975,259,288)
qt(0.975,547)
# Load bindata package
library(bindata)
library(tidyverse)
# Set seed so results are reproducible
set.seed(12345)
# CONTROL GROUP
# Specify mean response and correlation matrix
mean.ctrl <- c(0.10, 0.15, 0.20, 0.25)
corr.ctrl <- matrix(data = c(1.000, 0.500, 0.250, 0.125,
0.500, 1.000, 0.125, 0.250,
0.250, 0.125, 1.000, 0.500,
0.125, 0.250, 0.500, 1.000),
nrow = 4, byrow = TRUE)
# Generate random data
# You'll get a warning message you can ignore:
# In regularize.values(x, y, ties, missing(ties), na.rm = na.rm) : collapsing to unique 'x' values)
dat.ctrl <- rmvbin(n = 100, margprob = mean.ctrl, bincorr = corr.ctrl)
# Load bindata package
library(bindata)
install.packages("bindata")
# Load bindata package
library(bindata)
# Generate random data
# You'll get a warning message you can ignore:
# In regularize.values(x, y, ties, missing(ties), na.rm = na.rm) : collapsing to unique 'x' values)
dat.ctrl <- rmvbin(n = 100, margprob = mean.ctrl, bincorr = corr.ctrl)
# Add random ID to your data
# Add treatment group (A=0) to your data
dat.ctrl <- as.data.frame(dat.ctrl) %>%  # rmvbin generates a matrix; let's make it a dataframe
mutate(id = as.character(1:n()),       # Add ID
A = 0) %>%                      # Treatment = 0
gather(Visit, y, -id, -A) %>%          # Reshape to long format
mutate(Visit = as.numeric(substr(Visit, 2, 2)))
# TREATMENT GROUP
# Specify mean response and correlation matrix
mean.trt <- c(0.1, 0.2, 0.3, 0.4)
corr.trt <- matrix(data = c(1.000, 0.500, 0.250, 0.125,
0.500, 1.000, 0.125, 0.250,
0.250, 0.125, 1.000, 0.500,
0.125, 0.250, 0.500, 1.000),
nrow = 4, byrow = TRUE)
# Generate random data
# You'll get a warning message you can ignore:
# In regularize.values(x, y, ties, missing(ties), na.rm = na.rm) : collapsing to unique 'x' values)
dat.trt <- rmvbin(n = 100, margprob = mean.trt, bincorr = corr.trt)
# Add random ID to your data
# Add treatment group (A=0) to your data
dat.trt <- as.data.frame(dat.trt) %>%    # rmvbin generates a matrix; let's make it a dataframe
mutate(id = as.character(100 + 1:n()), # Add ID
A = 1) %>%                      # Treatment = 1
gather(Visit, y, -id, -A) %>%          # Reshape to long format
mutate(Visit = as.numeric(substr(Visit, 2, 2)))
# Join dataframes
data <- bind_rows(dat.ctrl, dat.trt) %>%
mutate(time = case_when(Visit == 1 ~ 0,
Visit == 2 ~ 6,
Visit == 3 ~ 12,
Visit == 4 ~ 18))
View(data)
tapply(data$y,list(data$A,data$time),mean)
prevalence <- tapply(data$y,list(data$A,data$time),mean)
View(prevalence)
prevalence <- as.dataframe(tapply(data$y,list(data$A,data$time),mean))
prevalence <- as.data.frame(tapply(data$y,list(data$A,data$time),mean))
View(prevalence)
View(data)
prevalence[3,] <- c(0, 6, 12, 18)
View(prevalence)
rownames(prevalence, c("Control","Treatment","Months"))
View(prevalence)
rownames(prevalence) <- c("Control","Treatment","Months")
View(prevalence)
library(ggplot2)
ggplot(prevalence, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
labs(x = "Months", y = "Values") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
ggtitle("Control vs. Treatment") +
theme_minimal()
prevalence$Months
prevalence <- transpose(prevalence)
View(prevalence)
prevalence <- as.data.frame(tapply(data$y,list(data$A,data$time),mean))
prevalence[3,] <- c(0, 6, 12, 18)
rownames(prevalence) <- c("Control","Treatment","Months")
View(prevalence)
p <- t(prevalence)
View(p)
p <- as.data.frame(t(prevalence))
View(p)
ggplot(p, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
View(p)
View(data)
state.summaries <-
data %>%
group_by(time) %>%
summarise(prevalence = mean(y))
View(state.summaries)
?group_by()
state.summaries <-
data %>%
group_by(time,A) %>%
summarise(prevalence = mean(y))
View(state.summaries)
View(p)
state.summaries <-
data %>%
group_by(time,A) %>%
summarise(prevalence = mean(y)) %>%
ggplot(aes(x = Months))+
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
state.summaries <-
data %>%
group_by(time,A) %>%
summarise(prevalence = mean(y))
state.summaries <-
data %>%
group_by(time,A) %>%
summarise(prevalence = mean(y),
sd = sd(y))
ggplot(p, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_pointrange(aes(y = Control, ymin = prevalence - 2*sd/sqrt(100),
ymax = prevalence + 2*sd/sqrt(100))) +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
geom_pointrange(aes(y = Treatment, ymin = prevalence - 2*sd/sqrt(100),
ymax = prevalence + 2*sd/sqrt(100))) +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
state.summaries %>%
ggplot(p, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_pointrange(aes(y = Control, ymin = prevalence - 2*sd/sqrt(100),
ymax = prevalence + 2*sd/sqrt(100))) +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
geom_pointrange(aes(y = Treatment, ymin = prevalence - 2*sd/sqrt(100),
ymax = prevalence + 2*sd/sqrt(100))) +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
state.summaries %>%
ggplot(aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_pointrange(aes(y = Control, ymin = prevalence - 2*sd/sqrt(100),
ymax = prevalence + 2*sd/sqrt(100))) +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
geom_pointrange(aes(y = Treatment, ymin = prevalence - 2*sd/sqrt(100),
ymax = prevalence + 2*sd/sqrt(100))) +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
ggplot(p, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_pointrange(aes(y = Control, ymin = prevalence - 2*state.summaries$sd/sqrt(100),
ymax = prevalence + 2*state.summaries$sd/sqrt(100))) +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
geom_pointrange(aes(y = Treatment, ymin = prevalence - 2*state.summaries$sd/sqrt(100),
ymax = prevalence + 2*state.summaries$sd/sqrt(100))) +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
ggplot(p, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_pointrange(aes(y = Control, ymin = Control - 2*state.summaries$sd/sqrt(100),
ymax = Control + 2*state.summaries$sd/sqrt(100))) +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
geom_pointrange(aes(y = Treatment, ymin = Treatment - 2*state.summaries$sd/sqrt(100),
ymax = Treatment + 2*state.summaries$sd/sqrt(100))) +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
ggplot(p, aes(x = Months)) +
geom_line(aes(y = Control, color = "Control"), linetype = "solid") +
geom_ribbon(aes( ymin = Control - 2*state.summaries$sd/sqrt(100),
ymax = Control + 2*state.summaries$sd/sqrt(100))) +
geom_line(aes(y = Treatment, color = "Treatment"), linetype = "solid") +
geom_ribbon(aes( ymin = Treatment - 2*state.summaries$sd/sqrt(100),
ymax = Treatment + 2*state.summaries$sd/sqrt(100))) +
labs(x = "Months", y = "Prevalence of little to no disability") +
scale_color_manual(values = c("Control" = "blue", "Treatment" = "red")) +
theme_minimal()
pnbinom(2,1,0.7)
？pnbinom()
?pnbinom
?qt()
qt(0.975,df=199)
qt(0.975,df=399)
pnorm(0.55,1,1)
pnorm(0.55,-1,1)
pt(3.031, df=173)
1-pt(3.031, df=173)
qt(0.05, df=172)
qt(0.95, df=172)
qt(0.975, df=172)
pt(3.031, df=172)
1-2*pt(3.031, df=172)
2*(1-pt(3.031, df=172))
pgamma(2*log(2), shape = 2, rate = 0.5)
knitr::opts_chunk$set(echo = TRUE)
alpha <- 0.05
n <- 10
lambda <- 1
lower_critical_value <- qpois(alpha / 2, lambda * n)
upper_critical_value <- qpois(1 - alpha / 2, lambda * n)
critical_region = c(lower_critical_value, upper_critical_value)
critical_ region
alpha <- 0.05
n <- 10
lambda <- 1
lower_critical_value <- qpois(alpha / 2, lambda * n)
upper_critical_value <- qpois(1 - alpha / 2, lambda * n)
critical_region = c(lower_critical_value, upper_critical_value)
critical_region
alpha <- 0.05
n <- 10
lambda <- 1
lower_critical_value <- qpois(alpha / 2, lambda * n)
upper_critical_value <- qpois(1 - alpha / 2, lambda * n)
critical_region = c(lower_critical_value, upper_critical_value)
critical_region
sig.level(critical_region, n, 0.5)
p_value <- ppois(critical_region, lambda * n)
p_value
alpha <- 0.05
n <- 10
lambda <- 1
lower_critical_value <- qpois(alpha / 2, lambda * n)
upper_critical_value <- qpois(1 - alpha / 2, lambda * n)
critical_region = c(lower_critical_value, upper_critical_value)
critical_region
set.seed(231020)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
lower_critical_value <- qpois(alpha / 2, n * lambda)
upper_critical_value <- qpois(1 - alpha / 2, n * lambda)
in_critical_region <- (test_statistic < lower_critical_value) ||
(test_statistic > upper_critical_value)
if (in_critical_region) {
count_in_critical_region <- count_in_critical_region + 1
}}
proportion_in_critical_region <- count_in_critical_region / n_simulations
proportion_in_critical_region
# Set the parameters
n_simulations <- 10000  # Number of simulations
sample_size <- 10  # Sample size
lambda <- 1  # True value of λ under the null hypothesis
alpha <- 0.05  # Nominal significance level
# Function to calculate Tn(Y) for a given sample
calculate_Tn <- function(sample) {
return(sum(sample))
}
# Initialize a counter for how many times Tn(Y) falls in the critical region
count_in_critical_region <- 0
# Perform the simulations
for (i in 1:n_simulations) {
# Simulate data from a Poisson(λ) distribution
simulated_data <- rpois(sample_size, lambda)
# Calculate the test statistic Tn(Y) for the simulated data
test_statistic <- calculate_Tn(simulated_data)
# Calculate the critical values based on the null distribution
lower_critical_value <- qpois(alpha / 2, sample_size * lambda)
upper_critical_value <- qpois(1 - alpha / 2, sample_size * lambda)
# Check if the test statistic falls in the critical region
in_critical_region <- (test_statistic < lower_critical_value) || (test_statistic > upper_critical_value)
# Update the counter if the test statistic is in the critical region
if (in_critical_region) {
count_in_critical_region <- count_in_critical_region + 1
}
}
# Calculate the proportion of times the test statistic falls in the critical region
proportion_in_critical_region <- count_in_critical_region / n_simulations
# Print the results
cat("Proportion in Critical Region:", proportion_in_critical_region, "\n")
set.seed(231020)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
lower_critical_value <- qpois(alpha / 2, n * lambda)
upper_critical_value <- qpois(1 - alpha / 2, n * lambda)
in_critical_region <- (test_statistic < lower_critical_value) |
(test_statistic > upper_critical_value)
if (in_critical_region) {
count_in_critical_region <- count_in_critical_region + 1
}}
proportion_in_critical_region <- count_in_critical_region / n_simulations
proportion_in_critical_region
set.seed(231020)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
if (test_statistic < lower_critical_value) | (test_statistic > upper_critical_value) {
set.seed(231020)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
if (test_statistic < lower_critical_value | (test_statistic > upper_critical_value) {
set.seed(231020)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
if (test_statistic < lower_critical_value | test_statistic > upper_critical_value) {
count_in_critical_region <- count_in_critical_region + 1
}}
proportion_in_critical_region <- count_in_critical_region / n_simulations
proportion_in_critical_region
set.seed(231021)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
if (test_statistic < lower_critical_value | test_statistic > upper_critical_value) {
count_in_critical_region <- count_in_critical_region + 1
}}
proportion_in_critical_region <- count_in_critical_region / n_simulations
proportion_in_critical_region
set.seed(231021)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, lambda)
test_statistic <- sum(simulated_data)
if (test_statistic <= lower_critical_value | test_statistic >= upper_critical_value) {
count_in_critical_region <- count_in_critical_region + 1
}}
proportion_in_critical_region <- count_in_critical_region / n_simulations
proportion_in_critical_region
set.seed(231021)
n_simulations <- 10000
count_in_critical_region <- 0
for (i in 1:n_simulations) {
simulated_data <- rpois(n, 2)
test_statistic <- sum(simulated_data)
if (test_statistic <= lower_critical_value | test_statistic >= upper_critical_value) {
count_in_critical_region <- count_in_critical_region + 1
}}
proportion_in_critical_region <- count_in_critical_region / n_simulations
proportion_in_critical_region
#| warning: false
# Install all the required packages and load the libraries needed in the following analysis
library(GEOquery)
install.packages("GEOquery")
knitr::opts_chunk$set(echo = TRUE)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("GEOquery")
BiocManager::install("DESeq2")
BiocManager::install("GOstats")
BiocManager::install("GO.db")
BiocManager::install("Category")
BiocManager::install("org.Hs.eg.db")
library(GEOquery)
#| warning: false
# Install all the required packages and load the libraries needed in the following analysis
library(GEOquery)
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
#| warning: false
# Install all the required packages and load the libraries needed in the following analysis
library(GEOquery)
library(DESeq2)
library(cluster)
library(pheatmap)
install.packages(c("flexdashboard", "DT", "shiny"))
install.packages("rsconnect")
setwd("~/Desktop/JHU/Statistical Programming Paradigms and Workflows/Project4")
